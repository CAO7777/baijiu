import os
import io
import json
import base64
import random
import hashlib
import pickle
import threading
import time
import math
import re
import ast
from queue import Queue
from pathlib import Path
from collections import Counter
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from flask import Flask, request, jsonify, render_template, url_for, Response
from flask_cors import CORS
from openai import OpenAI
from dotenv import load_dotenv

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib.ticker as mticker

# --- Matplotlib Font Setup for Chinese (No changes here) ---
font_candidates = [
    Path('simhei.ttf'), Path('fonts/simhei.ttf'),
    Path('/System/Library/Fonts/PingFang.ttc'), Path('/System/Library/Fonts/PingFang SC.ttc'),
    Path('C:/Windows/Fonts/simhei.ttf'), Path('C:/Windows/Fonts/msyh.ttc'),
]
fallback_families = ['PingFang SC', 'Heiti SC', 'Microsoft YaHei', 'SimHei', 'Noto Sans CJK SC', 'Source Han Sans SC', 'Arial Unicode MS']

def _find_font_from_families(families):
    for family in families:
        try:
            located = fm.findfont(fm.FontProperties(family=family), fallback_to_default=False)
            if Path(located).exists(): return Path(located)
        except Exception: continue
    return None

font_path = next((p for p in font_candidates if p.exists()), _find_font_from_families(fallback_families))
if font_path:
    try:
        fm.fontManager.addfont(str(font_path))
        font_prop = fm.FontProperties(fname=str(font_path))
        resolved_name = font_prop.get_name()
        plt.rcParams['font.sans-serif'] = [resolved_name, *fallback_families]
        plt.rcParams['axes.unicode_minus'] = False
        print(f"ä¸­æ–‡å›¾è¡¨å­—ä½“è®¾ç½®æˆåŠŸ: {resolved_name} ({font_path})")
    except Exception as exc:
        print(f"è­¦å‘Š: ä¸­æ–‡å­—ä½“åŠ è½½å¤±è´¥: {exc}")
else:
    print("è­¦å‘Š: æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“æ–‡ä»¶ã€‚")

# --- Basic Configuration ---
load_dotenv()
backend_env = Path('backend/.env')
if backend_env.exists(): load_dotenv(backend_env, override=True)
else: print("è­¦å‘Š: æœªæ‰¾åˆ° backend/.envï¼Œå°†ä½¿ç”¨é»˜è®¤ç¯å¢ƒå˜é‡ã€‚")

api_key = os.getenv("API_KEY")
api_url = os.getenv("API_URL")
if not api_key: raise RuntimeError("æœªæ£€æµ‹åˆ° API_KEYï¼Œè¯·åœ¨ .env æˆ– backend/.env ä¸­è¿›è¡Œé…ç½®ã€‚")

app = Flask(__name__)
CORS(app)
client = OpenAI(api_key=api_key, base_url=api_url)

# --- ### NEW: Real Data Loading and Preprocessing ### ---
# Global variable to hold our processed real data
# --- ### NEW: Real Data Loading and Preprocessing (Excel Version + City Cleaning) ### ---
# å…¨å±€å˜é‡ï¼Œç”¨äºä¿å­˜å¤„ç†åçš„çœŸå®æ•°æ®
REAL_DATA_DF = pd.DataFrame()
UNIQUE_CITIES = []

def load_and_preprocess_data():
    global REAL_DATA_DF, UNIQUE_CITIES
    
    data_path = Path("real_survey_data.xlsx")
    if not data_path.exists():
        print(f"é”™è¯¯: æœªåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰¾åˆ° 'real_survey_data.xlsx'ã€‚")
        print("è¯·ç¡®ä¿ä½ å·²å°†åŸå§‹ Excel æ–‡ä»¶å¤åˆ¶åˆ°é¡¹ç›®ç›®å½•å¹¶é‡å‘½åã€‚")
        return

    print("å¼€å§‹åŠ è½½å¹¶é¢„å¤„ç†çœŸå®è°ƒç ”æ•°æ® (Excel)...")
    
    try:
        df = pd.read_excel(data_path)
    except Exception as e:
        print(f"è¯»å– Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        print("è¯·ç¡®ä¿ 'openpyxl' å·²ç»é€šè¿‡ 'pip install openpyxl' å®‰è£…ã€‚")
        return

    column_mapping = {
        '2. æ€§åˆ«': 'æ€§åˆ«',
        '3. å¹´é¾„': 'å¹´é¾„',
        '4. èŒä¸š': 'èŒä¸š',
        '5. æ•™è‚²ç¨‹åº¦': 'æ•™è‚²ç¨‹åº¦',
        '6. æ”¶å…¥åŒºé—´': 'æ”¶å…¥åŒºé—´',
        '7. åŸå¸‚': 'åŸå¸‚', # This is the column we need to clean
        '8. MBTIï¼ˆæŒ‰ç…§æ‚¨å·²çŸ¥çš„è¿›è¡Œé€‰æ‹©ï¼‰       å¤–å‘(E) ä¸ å†…å‘(I): æè¿°èƒ½é‡è·å–æ–¹å¼ï¼Œå¤–å‘å‹ä»ç¤¾äº¤ä¸­è·å–èƒ½é‡ï¼Œå†…å‘å‹ä»ç‹¬å¤„ä¸­æ¢å¤èƒ½é‡    ': 'mbti_ei',
        '   æ„Ÿè§‰(S) ä¸ ç›´è§‰(N): æè¿°ä¿¡æ¯å¤„ç†æ–¹å¼ï¼Œæ„Ÿè§‰å‹å…³æ³¨ç»†èŠ‚å’Œç°å®ï¼Œç›´è§‰å‹å…³æ³¨æ•´ä½“å’Œæœªæ¥å¯èƒ½æ€§': 'mbti_sn',
        '   æ€è€ƒ(T) ä¸ æƒ…æ„Ÿ(F): æè¿°å†³ç­–æ–¹å¼ï¼Œæ€è€ƒå‹ä¾èµ–é€»è¾‘åˆ†æï¼Œæƒ…æ„Ÿå‹è€ƒè™‘ä¸ªäººä»·å€¼è§‚å’Œä»–äººæ„Ÿå—ã€‚': 'mbti_tf',
        '   åˆ¤æ–­(J) ä¸ çŸ¥è§‰(P): æè¿°ç”Ÿæ´»æ€åº¦ï¼Œåˆ¤æ–­å‹åå¥½æœ‰è®¡åˆ’æœ‰æ¡ç†ï¼ŒçŸ¥è§‰å‹æ›´çµæ´»å¼€æ”¾ã€‚': 'mbti_jp',
        '9. é¥®é…’é¢‘ç‡': 'é¥®é…’é¢‘ç‡',
        '10. é¥®é…’å†å²ï¼ˆé…’é¾„ï¼‰': 'é…’é¾„',
        '11. æœŸæœ›å•ç“¶ä»·æ ¼åŒºé—´': 'ç™½é…’ä»·æ ¼',
        '12. é¦™å‹ç±»åˆ«': 'é¦™å‹',
        '22. ä¸»è¦ç”¨é€”': 'ç”¨é€”'
    }
    
    df.rename(columns=column_mapping, inplace=True)
    
    # --- *** CITY CLEANING STEP *** ---
    # Convert to string, handle potential errors, split by '-', take the first part
    df['åŸå¸‚'] = df['åŸå¸‚'].astype(str).apply(lambda x: x.split('-')[0] if pd.notna(x) else 'æœªçŸ¥')
    # --- *** END CITY CLEANING *** ---

    df['ç”¨æˆ·ID'] = [f"User_{i+1:04d}" for i in range(len(df))]

    df['mbti_ei'] = df['mbti_ei'].astype(str).str.extract(r'\((\w)\)').fillna('I')
    df['mbti_sn'] = df['mbti_sn'].astype(str).str.extract(r'\((\w)\)').fillna('S')
    df['mbti_tf'] = df['mbti_tf'].astype(str).str.extract(r'\((\w)\)').fillna('T')
    df['mbti_jp'] = df['mbti_jp'].astype(str).str.extract(r'\((\w)\)').fillna('P')
    df['MBTI/æ€§æ ¼'] = df['mbti_ei'] + df['mbti_sn'] + df['mbti_tf'] + df['mbti_jp']
    
    df['é¦™å‹'] = df['é¦™å‹'].astype(str).apply(lambda x: x.split(' ')[0] if pd.notna(x) else 'æœªçŸ¥')
    
    final_columns = list(set(column_mapping.values()) | {'ç”¨æˆ·ID', 'MBTI/æ€§æ ¼'})
    
    for col in final_columns:
        if col not in df.columns:
            print(f"è­¦å‘Šï¼šæ•°æ®ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ— '{col}'ï¼Œå°†ç”¨ 'æœªçŸ¥' å¡«å……ã€‚")
            df[col] = "æœªçŸ¥"
            
    df = df[final_columns]
    
    # Fill any remaining NaNs after cleaning steps
    df.fillna('æœªçŸ¥', inplace=True) 
    
    # Drop rows where critical info might still be 'æœªçŸ¥' if needed, but fillna is usually safer
    # df.dropna(inplace=True) 

    df = df.reset_index(drop=True)
    
    REAL_DATA_DF = df
    UNIQUE_CITIES = sorted(REAL_DATA_DF['åŸå¸‚'].astype(str).unique().tolist())
    
    # Remove 'æœªçŸ¥' if it sneaked into unique cities
    if 'æœªçŸ¥' in UNIQUE_CITIES:
        UNIQUE_CITIES.remove('æœªçŸ¥')

    print(f"çœŸå®æ•°æ®åŠ è½½æˆåŠŸï¼å…±å¤„ç† {len(REAL_DATA_DF)} æ¡æœ‰æ•ˆç”¨æˆ·æ•°æ®ã€‚")
    print(f"å‘ç° {len(UNIQUE_CITIES)} ä¸ªç‹¬ç«‹åŸå¸‚ (å·²æ¸…ç†)ã€‚")


# --- Vector Search (TF-IDF Version) - MODIFIED FOR REAL DATA ---
VECTOR_DB_PATH = Path("vector_db/tfidf_database_real.pkl") # Use a new DB file

def row_to_text_real_data(row):
    # This function now uses the cleaned columns from the real data
    return (
        f"ç”¨æˆ·ç”»åƒï¼šæ€§åˆ« {row.get('æ€§åˆ«', 'æœªçŸ¥')}ï¼Œå¹´é¾„æ®µ {row.get('å¹´é¾„', 'æœªçŸ¥')}ï¼Œæ¥è‡ª {row.get('åŸå¸‚', 'æœªçŸ¥')} çš„ {row.get('èŒä¸š', 'æœªçŸ¥')}ã€‚"
        f"æ•™è‚²ç¨‹åº¦ä¸º {row.get('æ•™è‚²ç¨‹åº¦', 'æœªçŸ¥')}ï¼Œå¹´æ”¶å…¥ {row.get('æ”¶å…¥åŒºé—´', 'æœªçŸ¥')}ã€‚MBTIæ€§æ ¼æ˜¯ {row.get('MBTI/æ€§æ ¼', 'æœªçŸ¥')}ã€‚"
        f"é¥®é…’ä¹ æƒ¯ï¼šé¢‘ç‡ {row.get('é¥®é…’é¢‘ç‡', 'æœªçŸ¥')}ï¼Œé…’é¾„ {row.get('é…’é¾„', 'æœªçŸ¥')}ï¼Œåå¥½ {row.get('é¦™å‹', 'æœªçŸ¥')} ç™½é…’ï¼Œ"
        f"å¿ƒç†ä»·ä½åœ¨ {row.get('ç™½é…’ä»·æ ¼', 'æœªçŸ¥')}ï¼Œä¸»è¦ç”¨äº {row.get('ç”¨é€”', 'æœªçŸ¥')}ã€‚"
    )

def vectorize_database():
    if VECTOR_DB_PATH.exists():
        print("TF-IDF çœŸå®æ•°æ®å‘é‡åº“å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½ã€‚")
        return

    if REAL_DATA_DF.empty:
        print("é”™è¯¯: çœŸå®æ•°æ®æœªèƒ½åŠ è½½ï¼Œæ— æ³•åˆ›å»ºå‘é‡åº“ã€‚")
        return

    print("æœªæ‰¾åˆ°çœŸå®æ•°æ®å‘é‡åº“ï¼Œå¼€å§‹é¦–æ¬¡åˆ›å»º...")
    
    user_texts = REAL_DATA_DF.apply(row_to_text_real_data, axis=1).tolist()
    vectorizer = TfidfVectorizer()
    user_vectors = vectorizer.fit_transform(user_texts)
    
    VECTOR_DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    database = {'vectorizer': vectorizer, 'vectors': user_vectors, 'ids': REAL_DATA_DF['ç”¨æˆ·ID'].tolist()}
    
    with open(VECTOR_DB_PATH, 'wb') as f:
        pickle.dump(database, f)
    
    print("TF-IDF çœŸå®æ•°æ®å‘é‡åº“åˆ›å»ºæˆåŠŸï¼")

def find_similar_users_knn(persona, top_n=5):
    if not VECTOR_DB_PATH.exists() or REAL_DATA_DF.empty:
        return pd.DataFrame(), {}
        
    with open(VECTOR_DB_PATH, 'rb') as f:
        database = pickle.load(f)
        
    vectorizer, user_vectors, user_ids = database['vectorizer'], database['vectors'], database['ids']
    
    # The persona from the generator still uses the simple keys
    persona_text = (
        f"ç”¨æˆ·ç”»åƒï¼šæ€§åˆ« {persona.get('gender', '')}ï¼Œå¹´é¾„ {persona.get('age', '')}å²ï¼Œæ¥è‡ª {persona.get('city', '')} çš„ {persona.get('profession', '')}ã€‚"
        f"MBTIæ€§æ ¼æ˜¯ {persona.get('mbti', '')}ã€‚å…¶ä»–åå¥½ï¼šæ•™è‚²ç¨‹åº¦ {persona.get('education', '')}ï¼Œå¹´æ”¶å…¥ {persona.get('income', '')}ï¼Œ"
        f"å¿ƒç†ä»·ä½ {persona.get('expected_price', '')}ï¼Œåå¥½é¦™å‹ {persona.get('preferred_aroma', '')}ã€‚"
    )
    
    persona_vector = vectorizer.transform([persona_text])
    similarities = cosine_similarity(persona_vector, user_vectors).flatten()
    top_indices = np.argsort(similarities)[-top_n:][::-1]
    
    matched_user_ids = [user_ids[i] for i in top_indices]
    top_df = REAL_DATA_DF[REAL_DATA_DF['ç”¨æˆ·ID'].isin(matched_user_ids)].copy()
    
    # Ensure order is preserved
    top_df['__sort__'] = pd.Categorical(top_df['ç”¨æˆ·ID'], categories=matched_user_ids, ordered=True)
    top_df = top_df.sort_values('__sort__').drop('__sort__', axis=1)
    
    insights = {
        'top_aroma': top_df['é¦™å‹'].mode()[0] if not top_df['é¦™å‹'].mode().empty else "æœªçŸ¥",
        'price_band': top_df['ç™½é…’ä»·æ ¼'].mode()[0] if not top_df['ç™½é…’ä»·æ ¼'].mode().empty else "æœªçŸ¥",
        'typical_usage': top_df['ç”¨é€”'].mode()[0] if not top_df['ç”¨é€”'].mode().empty else "æœªçŸ¥"
    }
    return top_df, insights

# --- Real-time Streaming, Report Generation, Charting, etc. ---
# This entire section remains largely the same, as its logic is sound.
# The only change is how `find_similar_users_knn` gets its data.
def allocate_counts_from_ratio(ratio_map, total):
    if total <= 0 or not ratio_map:
        return {}
    items = []
    # Ensure all keys are strings, handle potential None ratios
    for key, ratio in ratio_map.items():
        try:
            ratio_value = float(ratio) if ratio is not None else 0.0
        except (TypeError, ValueError):
            ratio_value = 0.0
        
        # Ensure key is usable (string)
        str_key = str(key) if key is not None else "None" 

        exact = ratio_value * total / 100
        base = math.floor(exact)
        remainder = exact - base
        items.append([str_key, base, remainder]) # Use str_key

    assigned = sum(item[1] for item in items)
    remaining = max(0, total - assigned)
    items.sort(key=lambda entry: entry[2], reverse=True)
    
    idx = 0
    while remaining > 0 and items: # Add check for empty items list
        items[idx % len(items)][1] += 1
        idx += 1
        remaining -= 1
        
    return {key: count for key, count, _ in items}
analysis_queues = {}

def long_running_analysis(job_id, personas, product_data, persona_file=None):
    q = analysis_queues[job_id]
    try:
        persona_list = personas or []
        if persona_file:
            try:
                with open(persona_file, 'r', encoding='utf-8') as f:
                    file_personas = json.load(f)
                    if isinstance(file_personas, list) and file_personas:
                        persona_list = file_personas
            except Exception as exc:
                print(f"è­¦å‘Š: æ— æ³•ä» {persona_file} è¯»å–ç”»åƒï¼š{exc}ï¼Œç»§ç»­ä½¿ç”¨è¯·æ±‚ä¸­çš„æ•°æ®ã€‚")
        if not persona_list:
            raise ValueError("æœªæä¾›ä»»ä½•ç”»åƒæ•°æ®ã€‚")

        product_description = (product_data or {}).get('description', '')
        image_payload = (product_data or {}).get('image', '')
        base64_image = ''
        if isinstance(image_payload, str) and ',' in image_payload:
            base64_image = image_payload.split(',', 1)[1]
        elif isinstance(image_payload, str):
            base64_image = image_payload
        
        all_decision_data = []

        # --- ### ğŸ’¡ Stage 1: æ‰¹é‡å†³ç­– (å·²æŒ‰è¦æ±‚é‡æ„) ### ---
        # ç°åœ¨çš„å†³ç­–å°†åŸºäº K-NN ç›¸ä¼¼çœŸå®ç”¨æˆ·æ•°æ®
        
        batch_size = 5 # å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œå› ä¸ºæ¯ä¸ªç”»åƒçš„ prompt ç°åœ¨æ›´å¤æ‚
        total_personas = len(persona_list)
        print(f"[{job_id}] Progress: å¯åŠ¨å¿«é€Ÿå†³ç­–å¼•æ“...æ€»å…± {total_personas} ä½æ•°å­—äººã€‚")

        for i in range(0, total_personas, batch_size):
            batch_personas = persona_list[i:i + batch_size]
            
            persona_summaries_for_prompt = []
            
            # ğŸ’¡ å…³é”®ä¿®å¤ï¼šåœ¨æ‰¹é‡å†³ç­–æ—¶ï¼Œä¸ºæ¯ä¸ªç”¨æˆ·æŸ¥æ‰¾ç›¸ä¼¼æ•°æ®
            for j, p in enumerate(batch_personas):
                persona_id = i + j + 1
                
                # 1. ç«‹å³æ‰§è¡Œ K-NN æœç´¢ (ä½¿ç”¨ Top 3 ä¿è¯ prompt ç®€æ´)
                top_matches, ai_insights = find_similar_users_knn(p, top_n=3)
                
                similarity_lines = ["  ã€ç›¸ä¼¼çœŸå®ç”¨æˆ·å‚è€ƒ (Top 3)ã€‘:"]
                if not top_matches.empty:
                    for _, row in top_matches.iterrows():
                        similarity_lines.append(
                            f"  - çœŸå®ç”¨æˆ· ({row['åŸå¸‚']}, {row['èŒä¸š']}) åå¥½: {row['é¦™å‹']}, ä»·ä½: {row['ç™½é…’ä»·æ ¼']}, ç”¨é€”: {row['ç”¨é€”']}ã€‚"
                        )
                    similarity_lines.append(f"  - æ ¸å¿ƒæ´å¯Ÿ: åå¥½{ai_insights.get('top_aroma', 'â€”')}, ä»·ä½{ai_insights.get('price_band', 'â€”')}")
                else:
                    similarity_lines.append("  - æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç›¸ä¼¼çœŸå®ç”¨æˆ·ã€‚")
                
                similar_users_text = "\n".join(similarity_lines)

                # 2. æ„å»ºåŒ…å«çœŸå®æ•°æ®å‚è€ƒçš„ prompt
                summary = (
                    f"--- ç”»åƒ ID {persona_id} ---\n"
                    f"  ã€æ•°å­—äººç”»åƒã€‘: {p.get('age')}å²{p.get('gender')}ï¼Œæ¥è‡ª{p.get('city', 'æœªçŸ¥')}çš„{p.get('profession', 'æœªçŸ¥')}ã€‚"
                    f"MBTI {p.get('mbti', 'æœªçŸ¥')}ï¼Œå¹´æ”¶å…¥{p.get('income', 'æœªçŸ¥')}ï¼Œåå¥½{p.get('preferred_aroma', 'æœªçŸ¥')}é¦™å‹ï¼Œ"
                    f"å¿ƒç†ä»·ä½{p.get('expected_price', 'æœªçŸ¥')}ã€‚\n"
                    f"{similar_users_text}"
                )
                persona_summaries_for_prompt.append(summary)

            # 3. æ„å»ºæ–°çš„æ‰¹é‡å†³ç­– Prompt
            batch_decision_prompt = (
                "ä½ æ˜¯ä¸€ä½é«˜æ•ˆçš„å¸‚åœºåˆ†æå¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯å¿«é€Ÿåˆ¤æ–­ä¸åŒæ¶ˆè´¹è€…å¯¹ä¸€æ¬¾æ–°äº§å“çš„è´­ä¹°æ„å‘ã€‚\n\n"
                f"äº§å“æ–‡å­—æè¿°ï¼š'{product_description}'\näº§å“å›¾ç‰‡å·²æä¾›ã€‚\n\n"
                f"ä»¥ä¸‹æ˜¯æœ¬æ‰¹æ¬¡çš„ {len(batch_personas)} ä½æ¶ˆè´¹è€…ç”»åƒï¼Œä»¥åŠå¯¹åº”çš„å‡ ç»„ä¸ä»–ä»¬ç›¸ä¼¼çš„ã€çœŸå®ç”¨æˆ·æ•°æ®å‚è€ƒã€‘ï¼š\n\n" + "\n\n".join(persona_summaries_for_prompt) + "\n\n"
                "ä»»åŠ¡ï¼šè¯·ä¸ºæ¯ä¸€ä½æ¶ˆè´¹è€…åšå‡ºç‹¬ç«‹çš„è´­ä¹°å†³ç­–ã€‚ä½ çš„å†³ç­–å¿…é¡»ã€é‡ç‚¹å‚è€ƒã€‘æ¯ä¸ªç”»åƒä¸‹æ–¹æä¾›çš„ã€ç›¸ä¼¼çœŸå®ç”¨æˆ·å‚è€ƒã€‘æ•°æ®ï¼Œå¹¶ç»“åˆæ•°å­—äººç”»åƒå’Œäº§å“ä¿¡æ¯è¿›è¡Œåˆ¤æ–­ã€‚\n"
                "ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªé”® `decisions`ï¼Œå…¶å€¼ä¸ºä¸€ä¸ªæ•°ç»„ã€‚\n"
                "æ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡éƒ½å¿…é¡»åŒ…å«ä¸¤ä¸ªé”®ï¼š`persona_id` (æ•´æ•°) å’Œ `decision` (å­—ç¬¦ä¸² 'è´­ä¹°' æˆ– 'ä¸è´­ä¹°')ã€‚\n"
                "ä¾‹å¦‚: {\"decisions\": [{\"persona_id\": 1, \"decision\": \"è´­ä¹°\"}, {\"persona_id\": 2, \"decision\": \"ä¸è´­ä¹°\"}]}"
            )
            
            # 4. è°ƒç”¨å¤§æ¨¡å‹ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
            message_content = [{"type": "text", "text": batch_decision_prompt}]
            if base64_image:
                message_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})

            response = client.chat.completions.create(
                model="gpt-4o-mini", # ä½¿ç”¨ gpt-4o-mini ä¿æŒå¿«é€Ÿ
                response_format={"type": "json_object"},
                messages=[{"role": "user", "content": message_content}],
                max_tokens=1000
            )
            
            batch_decisions = json.loads(response.choices[0].message.content).get('decisions', [])
            
            for decision_info in batch_decisions:
                persona_id = decision_info.get('persona_id')
                decision = decision_info.get('decision')
                if persona_id and decision and 1 <= persona_id <= total_personas:
                    all_decision_data.append({
                        "persona_details": persona_list[persona_id - 1],
                        "persona_id": persona_id,
                        "decision": decision # ğŸ’¡ è¿™ä¸ªå†³ç­–ç°åœ¨æ˜¯åŸºäºçœŸå®æ•°æ®åšå‡ºçš„
                    })
            print(f"[{job_id}] Progress: å¿«é€Ÿå†³ç­–å®Œæˆ: {len(all_decision_data)} / {total_personas} ä½æ•°å­—äººã€‚")
            time.sleep(0.5)

        # --- ### Stage 2: ç”Ÿæˆæ•´ä½“å¸‚åœºæŠ¥å‘Š ### ---
        # ... (è¿™éƒ¨åˆ†ä»£ç ä¸éœ€è¦æ”¹å˜) ...
        print(f"[{job_id}] Progress: æ‰€æœ‰å†³ç­–å·²å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ•´ä½“å¸‚åœºåˆ†ææŠ¥å‘Š...")
        
        summary_context_lines = ["ä»¥ä¸‹æ˜¯å„æ¨¡æ‹Ÿç”¨æˆ·çš„æ ¸å¿ƒç”»åƒåŠå…¶æœ€ç»ˆè´­ä¹°å†³ç­–ï¼š"]
        for item in all_decision_data:
            p = item['persona_details']
            persona_summary = (
                f"- ç”¨æˆ· {item['persona_id']} ({item['decision']}): "
                f"{p.get('age')}å²{p.get('gender')}ï¼Œæ¥è‡ª{p.get('city', 'æœªçŸ¥åŸå¸‚')}ï¼ŒèŒä¸šä¸º{p.get('profession', 'æœªçŸ¥')}ï¼Œ"
                f"MBTIä¸º{p.get('mbti', 'æœªçŸ¥')}ï¼Œå¹´æ”¶å…¥{p.get('income', 'æœªçŸ¥')}ã€‚"
            )
            summary_context_lines.append(persona_summary)
        all_reports_text = "\n".join(summary_context_lines)

        structured_summary_prompt = (
            f"ä½ æ˜¯ä¸€ä½é¡¶çº§çš„å¸‚åœºç ”ç©¶æ€»ç›‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯åŸºäºä»¥ä¸‹ {len(all_decision_data)} ä»½æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®ï¼Œæ’°å†™ä¸€ä»½æ·±åˆ»ã€ä¸“ä¸šã€ç»“æ„åŒ–çš„ç»¼åˆå¸‚åœºåˆ†ææŠ¥å‘Šã€‚\n\n"
            f"é‡è¦ï¼šè¾“å‡ºä¸­ä»»ä½•åœ°æ–¹ä¸èƒ½åŒ…å«ä»»ä½•Markdownç¬¦å·ï¼ˆå¦‚`*`ã€`#`ã€`-`ï¼‰ã€‚"
            f"ã€åŸå§‹æ•°æ®ã€‘\n{all_reports_text}\n\n"
            f"è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹å¤§çº²æ’°å†™ä½ çš„ç»¼åˆåˆ†ææŠ¥å‘Šï¼Œç¡®ä¿å†…å®¹æ·±åˆ»ã€é€»è¾‘æ¸…æ™°ã€è¯­è¨€ä¸“ä¸šã€‚æŠ¥å‘Šä¸­ä¸èƒ½åŒ…å«ä»»ä½•Markdownç¬¦å·ã€‚\n\n"
            f"1. æ ¸å¿ƒæ´å¯Ÿä¸è´­ä¹°æ„å‘\n- é¦–å…ˆï¼Œç²¾ç¡®è®¡ç®—å¹¶æ˜ç¡®å±•ç¤ºæ€»ä½“è´­ä¹°ç‡ï¼ˆè´­ä¹°äººæ•°/æ€»äººæ•°ï¼‰ã€‚\n- æç‚¼å‡ºæœ¬æ¬¡æ¨¡æ‹Ÿä¸­æœ€å…³é”®ã€æœ€æ ¸å¿ƒçš„å¸‚åœºæ´å¯Ÿæ˜¯ä»€ä¹ˆã€‚\n\n"
            f"2. å…³é”®è´­ä¹°é©±åŠ¨åŠ›åˆ†æ\n- æ·±å…¥æ€»ç»“å¸å¼•ç”¨æˆ·åšå‡ºâ€œè´­ä¹°â€å†³ç­–çš„æ ¸å¿ƒå› ç´ ã€‚è¯·ä»äº§å“å±æ€§ã€åŒ…è£…è®¾è®¡ã€å“ç‰Œå®šä½å’Œä»·æ ¼æ„ŸçŸ¥ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œå‰–æã€‚\n\n"
            f"3. ä¸»è¦è´­ä¹°å£å’å‰–æ\n- åŒæ ·ï¼Œæ·±å…¥å‰–æå¯¼è‡´ç”¨æˆ·åšå‡ºâ€œä¸è´­ä¹°â€å†³ç­–çš„å…³é”®éšœç¢ã€‚åˆ†æè¿™äº›éšœç¢æ˜¯æºäºäº§å“è‡ªèº«ã€ä»·æ ¼ã€åŒ…è£…ï¼Œè¿˜æ˜¯ä¸ç›®æ ‡ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚å­˜åœ¨é”™ä½ã€‚\n\n"
            f"4. ç»¼åˆç»“è®ºä¸å¸‚åœºç­–ç•¥å»ºè®®\n- å¯¹äº§å“çš„å¸‚åœºæ½œåŠ›ç»™å‡ºä¸€ä¸ªç®€æ´æœ‰åŠ›çš„ç»¼åˆç»“è®ºã€‚\n- åŸºäºä»¥ä¸Šæ‰€æœ‰åˆ†æï¼Œä¸ºè¯¥äº§å“çš„å¸‚åœºç­–ç•¥æä¾›1-2æ¡å…·ä½“çš„ã€å¯æ‰§è¡Œçš„å»ºè®®ã€‚"
            f"å†æ¬¡å¼ºè°ƒé‡è¦ï¼šè¾“å‡ºä¸­ä»»ä½•åœ°æ–¹ä¸èƒ½åŒ…å«ä»»ä½•Markdownç¬¦å·ï¼ˆå¦‚`*`ã€`#`ã€`-`ï¼‰ã€‚"
        )
        
        summary_response = client.chat.completions.create(model="gpt-4-turbo", messages=[{"role": "user", "content": structured_summary_prompt}], max_tokens=1500)
        summary_report = summary_response.choices[0].message.content.replace('*', '').replace('#', '')
        q.put({"type": "summary_report", "data": summary_report})
        print(f"[{job_id}] Progress: æ•´ä½“æŠ¥å‘Šå·²ç”Ÿæˆå¹¶å‘é€ã€‚")
        time.sleep(1)


        # --- ### Stage 3: å›¾è¡¨å’Œè¡¨æ ¼åˆ†æ ### ---
        # ... (è¿™éƒ¨åˆ†ä»£ç ä¸éœ€è¦æ”¹å˜) ...
        print(f"[{job_id}] Progress: å¼€å§‹ç”Ÿæˆå›¾è¡¨åˆ†æ...")
        def generate_and_stream_chart_ux(chart_id, title, chart_type, data):
            chart_b64 = generate_chart_base64(chart_type, data, title)
            q.put({"type": "chart_and_table", "data": {"id": chart_id, "title": title, "chart": chart_b64, "table": data}})
            analysis = get_ai_analysis_for_table(title, data)
            q.put({"type": "table_analysis", "data": {"id": chart_id, "analysis": analysis}})
            time.sleep(0.5)

        def get_age_group(age):
            try: age = int(age)
            except (ValueError, TypeError): return "æœªçŸ¥å¹´é¾„"
            if age < 25: return "25å²ä»¥ä¸‹"
            if age < 30: return "25-29å²"
            if age < 35: return "30-34å²"
            if age < 40: return "35-39å²"
            if age < 45: return "40-44å²"
            if age < 50: return "45-49å²"
            return "50å²åŠä»¥ä¸Š"
        
        buying_results = [r for r in all_decision_data if r['decision'] == 'è´­ä¹°']
        non_buying_results = [r for r in all_decision_data if r['decision'] == 'ä¸è´­ä¹°']
        
        income_order = ["10ä¸‡ä»¥ä¸‹", "10-20ä¸‡", "20-50ä¸‡", "50ä¸‡ä»¥ä¸Š"]
        
        generate_and_stream_chart_ux("overall", "æ€»ä½“è´­ä¹°æ„å‘æ¯”ä¾‹", "pie", {'è´­ä¹°': len(buying_results), 'ä¸è´­ä¹°': len(non_buying_results)})

        if buying_results:
            buyer_gender_data = dict(Counter(p['persona_details']['gender'] for p in buying_results))
            if buyer_gender_data: generate_and_stream_chart_ux("buyer_gender", "è´­ä¹°ç”¨æˆ·æ€§åˆ«åˆ†å¸ƒ", "pie", buyer_gender_data)
            buyer_city_data = dict(Counter(p['persona_details']['city'] for p in buying_results if p['persona_details']['city']))
            if buyer_city_data: generate_and_stream_chart_ux("buyer_city", "è´­ä¹°ç”¨æˆ·åŸå¸‚åˆ†å¸ƒ", "bar", buyer_city_data)
            buyer_age_data = dict(Counter(get_age_group(p['persona_details']['age']) for p in buying_results))
            if buyer_age_data: generate_and_stream_chart_ux("buyer_age", "è´­ä¹°ç”¨æˆ·å¹´é¾„åˆ†å¸ƒ", "bar", buyer_age_data)
            buyer_mbti_data = dict(Counter(p['persona_details']['mbti'] for p in buying_results if p['persona_details']['mbti']))
            if buyer_mbti_data: generate_and_stream_chart_ux("buyer_mmbti", "è´­ä¹°ç”¨æˆ·MBTIåˆ†å¸ƒ", "pie", buyer_mbti_data)
            income_counts = Counter(p['persona_details']['income'] for p in buying_results if p['persona_details']['income'])
            if income_counts:
                sorted_income_keys = sorted(income_counts.keys())
                sorted_income = {key: income_counts[key] for key in sorted_income_keys}
                if sorted_income: generate_and_stream_chart_ux("buyer_income", "è´­ä¹°ç”¨æˆ·æ”¶å…¥åˆ†å¸ƒ", "pie", sorted_income)

        if non_buying_results:
            nonbuyer_gender_data = dict(Counter(p['persona_details']['gender'] for p in non_buying_results))
            if nonbuyer_gender_data: generate_and_stream_chart_ux("nonbuyer_gender", "æœªè´­ä¹°ç”¨æˆ·æ€§åˆ«åˆ†å¸ƒ", "pie", nonbuyer_gender_data)
        
        print(f"[{job_id}] Progress: å›¾è¡¨åˆ†æå®Œæˆã€‚")


        # --- ### ğŸ’¡ Stage 4: å¡«å……è¯¦ç»†çš„ä¸ªäººæŠ¥å‘Š (é‡æ„) ### ---
        # è¿™ä¸€æ­¥ç°åœ¨å°†ä½¿ç”¨ Stage 1 ä¸­åŸºäºçœŸå®æ•°æ®åšå‡ºçš„å†³ç­–
        
        print(f"[{job_id}] Progress: æ­£åœ¨åå°è¡¥å……æ¯ä¸ªæ•°å­—äººçš„è¯¦ç»†åˆ†æç†ç”±...")
        
        MAX_RETRIES = 10
        for item in all_decision_data:
            persona = item['persona_details']
            persona_id = item['persona_id']
            # ğŸ’¡ å…³é”®ï¼šè¿™æ˜¯æ¥è‡ª Stage 1 (å·²åŸºäºçœŸå®æ•°æ®) çš„å†³ç­–
            pre_determined_decision = item['decision']
            
            analysis_data = None
            for attempt in range(MAX_RETRIES):
                try:
                    # 1. å†æ¬¡æŸ¥æ‰¾ K-NN (è¿™æ¬¡ä½¿ç”¨ Top 5 ä»¥è·å¾—æ›´ä¸°å¯Œçš„æŠ¥å‘Š)
                    top_matches, ai_insights = find_similar_users_knn(persona, top_n=5)
                    
                    profession = persona.get('profession') or 'æœªå¡«å†™èŒä¸š'
                    base_lines = ["ã€åŸºç¡€ä¿¡æ¯ã€‘", f"- å¹´é¾„ï¼š{persona.get('age', 'æœªå¡«å†™')} å²", f"- æ€§åˆ«ï¼š{persona.get('gender')}", f"- å¸¸ä½åŸå¸‚ï¼š{persona.get('city', 'æœªå¡«å†™') or 'æœªå¡«å†™'}", f"- èŒä¸šï¼š{profession}", f"- MBTIï¼š{(persona.get('mbti') or '').upper()}"]
                    optional_lines = []
                    if persona.get('education'): optional_lines.append(f"- æ•™è‚²ç¨‹åº¦ï¼š{persona['education']}")
                    if persona.get('income'): optional_lines.append(f"- å¹´æ”¶å…¥åŒºé—´ï¼š{persona['income']}")
                    if persona.get('drink_frequency'): optional_lines.append(f"- é¥®é…’é¢‘ç‡ï¼š{persona['drink_frequency']}")
                    if persona.get('drinking_history'): optional_lines.append(f"- é¥®é…’å¹´é™ï¼š{persona['drinking_history']} å¹´")
                    if persona.get('expected_price'): optional_lines.append(f"- å¿ƒç†ä»·ä½ï¼š{persona['expected_price']}")
                    if persona.get('preferred_aroma'): optional_lines.append(f"- åå¥½é¦™å‹ï¼š{persona['preferred_aroma']}")

                    profile_sections = ["\n".join(base_lines)]
                    if optional_lines: profile_sections.append("ã€é¢å¤–ç”»åƒçº¿ç´¢ã€‘\n" + "\n".join(optional_lines))
                    
                    if not top_matches.empty:
                        similarity_lines = ["ã€ç›¸ä¼¼ç”¨æˆ·æ¶ˆè´¹è®°å½• (Top 5)ã€‘"]
                        for _, row in top_matches.iterrows():
                            similarity_lines.append(f"- {row['ç”¨æˆ·ID']}ï¼š{row['å¹´é¾„']} {row['æ€§åˆ«']}ï¼Œ{row['åŸå¸‚']}ï¼ŒèŒä¸š{row['èŒä¸š']}ï¼ŒMBTI {row['MBTI/æ€§æ ¼']}ï¼›åå¥½{row['é¦™å‹']}ï¼Œä»·ä½{row['ç™½é…’ä»·æ ¼']}ï¼Œç”¨é€”ï¼š{row['ç”¨é€”']}ã€‚")
                        insight_lines = ["ã€ç›¸ä¼¼ç”¨æˆ·è´­ä¹°æ´å¯Ÿã€‘", f"- æ ¸å¿ƒåå¥½é¦™å‹ï¼š{ai_insights.get('top_aroma', 'â€”')}", f"- æ ¸å¿ƒä»·æ ¼å¸¦ï¼š{ai_insights.get('price_band', 'â€”')}", f"- å¸¸è§ä½¿ç”¨åœºæ™¯ï¼š{ai_insights.get('typical_usage', 'â€”')}"]
                        profile_sections.extend(["\n".join(similarity_lines), "\n".join(insight_lines)])
                    else:
                        profile_sections.append("ã€ç›¸ä¼¼ç”¨æˆ·å‚è€ƒã€‘\n- æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è¶³å¤Ÿçš„åŒ¹é…ç”¨æˆ·ï¼Œä»¥ä¸‹åˆ†æå°†åŸºäºè¾“å…¥ç”»åƒè¿›è¡Œæ¨æ–­ã€‚")
                    
                    real_user_prompt = "\n\n".join(profile_sections)
                    
                    # 2. æ„å»ºè¯¦ç»†æŠ¥å‘Šçš„ Promptï¼Œä¾ç„¶ä¼ å…¥å·²ç¡®å®šçš„å†³ç­–
                    structured_individual_prompt = (
                        f"èƒŒæ™¯ï¼šä½ å°†ä»£å…¥ä»¥ä¸‹äººç‰©ç”»åƒè¿›è¡Œæ€è€ƒã€‚\näººç‰©ç”»åƒä¸ç›¸ä¼¼ç”¨æˆ·æ¶ˆè´¹è®°å½•ï¼š\n---\n{real_user_prompt}\n---\n\n"
                        f"äº§å“æ–‡å­—æè¿°ï¼š'{product_description}'\näº§å“å›¾ç‰‡å·²æä¾›ã€‚\n\n"
                        f"ä»»åŠ¡ï¼šå·²çŸ¥è¯¥ç”¨æˆ·çš„æœ€ç»ˆå†³ç­–æ˜¯ â€˜{pre_determined_decision}â€™ã€‚è¯·å›´ç»•è¿™ä¸ªæ—¢å®šå†³ç­–ï¼Œå®Œæˆä¸€ä»½è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚ä½ çš„æ‰€æœ‰åˆ†æã€è¯„åˆ†å’Œç†ç”±éƒ½å¿…é¡»ä¸ â€˜{pre_determined_decision}â€™ è¿™ä¸€æœ€ç»ˆç»“æœä¿æŒé€»è¾‘ä¸€è‡´ã€‚\n\n"
                        f"ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š\n"
                        f"1. `structured_report`: ä¸€ä¸ªåŒ…å«åˆ†ææ–‡æœ¬çš„å¯¹è±¡ï¼Œå¿…é¡»æœ‰`packaging_analysis`, `fit_analysis`, `scenario_analysis`ä¸‰ä¸ªé”®ã€‚\n"
                        f"2. `radar_scores`: ä¸€ä¸ªåŒ…å«åŒ¹é…åº¦è¯„åˆ†çš„å¯¹è±¡ï¼Œå¿…é¡»æœ‰`åŒ…è£…`, `ä»·æ ¼`, `é¦™å‹`, `åœºæ™¯`å››ä¸ªé”®ï¼Œæ¯ä¸ªé”®çš„å€¼ä¸º0-10çš„æ•´æ•°ã€‚\n"
                        f"3. `decision`: å­—ç¬¦ä¸²ï¼Œå…¶å€¼å¿…é¡»æ˜¯ '{pre_determined_decision}'ã€‚\n"
                        f"4. `reason`: å­—ç¬¦ä¸²ï¼Œå¯¹æœ€ç»ˆå†³ç­– â€˜{pre_determined_decision}â€™ çš„æ€»ç»“æ€§ç†ç”±ã€‚\n\n"
                        f"æ€è€ƒé“¾æŒ‡å¼•ï¼ˆåœ¨å†…å¿ƒå®Œæˆï¼Œä¸è¦è¾“å‡ºè¿‡ç¨‹ï¼‰ï¼š\n"
                        f"1. è§†è§‰åˆ†æï¼šè§‚å¯Ÿäº§å“å›¾ç‰‡ï¼Œè¯„ä¼°åŒ…è£…è®¾è®¡ã€é£æ ¼å’Œæ¡£æ¬¡æ„Ÿã€‚å°†æ­¤æ€è€ƒæ€»ç»“å†™å…¥ `structured_report.packaging_analysis`ã€‚\n"
                        f"2. å¥‘åˆåº¦åˆ†æï¼šç»“åˆäº§å“æè¿°ã€è§†è§‰åˆ†æå’Œã€ç›¸ä¼¼ç”¨æˆ·å‚è€ƒæ•°æ®ã€‘ï¼Œå¯¹æ¯”ä½ çš„äººè®¾ï¼Œè¯„ä¼°äº§å“åœ¨é¦™å‹ã€ä»·æ ¼ã€å“è´¨ç­‰æ–¹é¢æ˜¯å¦åŒ¹é…ã€‚å°†æ­¤æ€è€ƒæ€»ç»“å†™å…¥ `structured_report.fit_analysis`ã€‚\n"
                        f"3. åœºæ™¯æ„æ€ï¼šæ„æ€1-2ä¸ªä½ å¯èƒ½ä¼šä½¿ç”¨è¯¥äº§å“çš„å…·ä½“åœºæ™¯ã€‚å°†æ­¤æ€è€ƒæ€»ç»“å†™å…¥ `structured_report.scenario_analysis`ã€‚\n"
                        f"4. é‡åŒ–è¯„åˆ†ï¼šåŸºäºä»¥ä¸Šåˆ†æï¼Œä¸º`åŒ…è£…`ã€`ä»·æ ¼`ã€`é¦™å‹`ã€`åœºæ™¯`å››ä¸ªç»´åº¦ä¸ä½ äººè®¾çš„åŒ¹é…åº¦åˆ†åˆ«æ‰“åˆ†ï¼Œå¡«å…¥`radar_scores`ã€‚\n"
                        f"5. æœ€ç»ˆå†³ç­–ï¼šç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä¸ªå¼ºæœ‰åŠ›çš„`reason`æ¥æ”¯æ’‘å·²ç¡®å®šçš„å†³ç­– '{pre_determined_decision}'ã€‚\n"
                        f"å†æ¬¡å¼ºè°ƒé‡è¦ï¼šè¾“å‡ºä¸­ä»»ä½•åœ°æ–¹ä¸èƒ½åŒ…å«ä»»ä½•Markdownç¬¦å·ï¼ˆå¦‚`*`ã€`#`ã€`-`ï¼‰ã€‚"
                    )

                    # 3. è°ƒç”¨å¤§æ¨¡å‹ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
                    message_content = [{"type": "text", "text": structured_individual_prompt}]
                    if base64_image:
                        message_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})

                    response = client.chat.completions.create(
                        model="gpt-4o", # è¯¦ç»†æŠ¥å‘Šä½¿ç”¨æ›´å¼ºçš„ gpt-4o
                        response_format={"type": "json_object"},
                        messages=[{"role": "user", "content": message_content}],
                        max_tokens=2000
                    )
                    
                    raw_content = response.choices[0].message.content
                    if not raw_content or not raw_content.strip().startswith('{'):
                        raise json.JSONDecodeError("Response is not a valid JSON object.", raw_content, 0)
                    
                    analysis_data = json.loads(raw_content)
                    break 

                except (json.JSONDecodeError, IndexError) as e:
                    print(f"è­¦å‘Š: è§£æç”»åƒ {persona_id} çš„æŠ¥å‘Šå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES})ã€‚é”™è¯¯: {e}")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(1)
                    else:
                        print(f"é”™è¯¯: ç”»åƒ {persona_id} çš„æŠ¥å‘Šåœ¨ {MAX_RETRIES} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚è·³è¿‡æ­¤ç”»åƒã€‚")
            
            if analysis_data is None:
                continue
            
            report_text_obj = analysis_data.get("structured_report", {})
            cleaned_report_text = {k: str(v).replace('*', '').replace('#', '') for k, v in report_text_obj.items()}
            radar_scores = analysis_data.get("radar_scores", {})
            decision = analysis_data.get("decision", pre_determined_decision)
            reason = analysis_data.get("reason", "").replace('*', '').replace('#', '')
            radar_chart_b64 = generate_chart_base64('radar', radar_scores, "ç”»åƒ-äº§å“åŒ¹é…åº¦é›·è¾¾å›¾")

            result_package = {
                "type": "individual_report",
                "data": { 
                    "persona_id": persona_id,
                    "report": cleaned_report_text,
                    "final_decision": {"decision": decision, "reason": reason},
                    "radar_chart": radar_chart_b64,
                    "persona_details": persona,
                    "decision": decision 
                }
            }
            q.put(result_package)
            time.sleep(0.5)

    except Exception as e:
        print(f"Analysis thread error: {e}")
        import traceback
        traceback.print_exc()
        q.put({"type": "error", "data": str(e)})
    finally:
        q.put({"type": "done"})
        if job_id in analysis_queues: del analysis_queues[job_id]


def generate_chart_base64(chart_type, data, title):
    if not data or (chart_type == 'pie' and sum(data.values()) == 0): 
        return None
    
    fig = plt.figure(figsize=(9, 7))
    
    if chart_type == 'radar':
        ax = fig.add_subplot(111, polar=True)
    else:
        ax = fig.add_subplot(111)
        ax.set_title(title, pad=20, fontsize=16)

    if chart_type == 'pie':
        pie_labels = [f"{key} ({value}äºº)" for key, value in data.items()]
        ax.pie(data.values(), labels=pie_labels, autopct='%1.1f%%', startangle=120, textprops={'fontsize': 10})
        ax.axis('equal')

    elif chart_type in ['line', 'bar']:
        ax.bar(data.keys(), data.values(), color='#4A90E2')
        ax.set_ylabel("äººæ•°", fontsize=12)
        plt.setp(ax.get_xticklabels(), rotation=30, ha="right", fontsize=10)
        plt.setp(ax.get_yticklabels(), fontsize=10)
        ax.yaxis.set_major_locator(mticker.MaxNLocator(integer=True))
        ax.set_ylim(bottom=0)

    elif chart_type == 'radar':
        labels = list(data.keys())
        values = list(data.values())
        num_vars = len(labels)
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        values += values[:1]
        angles += angles[:1]
        ax = plt.subplot(111, polar=True)
        ax.plot(angles, values, color='#6E0F1A', linewidth=3, zorder=3)
        ax.fill(angles, values, color='#6E0F1A', alpha=0.25, zorder=2)
        ax.grid(color='lightgrey', linestyle='--', linewidth=0.7)
        ax.spines['polar'].set_visible(True)
        ax.spines['polar'].set_color('lightgrey')
        ax.set_yticks(np.arange(0, 11, 2))
        ax.set_yticklabels(["", "2", "4", "6", "8", "10"], color="grey", size=15)
        ax.set_ylim(0, 10)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, size=25, fontproperties=font_prop)
        ax.tick_params(axis='x', pad=25)

    plt.tight_layout(pad=2.5)
    buf = io.BytesIO()
    plt.savefig(buf, format='png', transparent=True, dpi=200)
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode('utf-8')

def get_ai_analysis_for_table(table_title, table_data):
    if not table_data: return "æ— æ•°æ®å¯ä¾›åˆ†æã€‚"
    prompt = f"ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯å…³äº '{table_title}' çš„æ•°æ®ï¼š{json.dumps(table_data, ensure_ascii=False)}ã€‚è¯·ç”¨ä¸€å¥è¯ç»™å‡ºæœ€æ ¸å¿ƒçš„å•†ä¸šæ´å¯Ÿã€‚"
    try:
        response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], max_tokens=150)
        return response.choices[0].message.content
    except Exception as e: return f"AIæ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}"

# --- ### NEW: API endpoint to provide cities to the frontend ### ---
COMPREHENSIVE_CITY_LIST = [
    # ç›´è¾–å¸‚ (Municipalities)
    'åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†',
    # å¹¿ä¸œçœ
    'å¹¿ä¸œ', 'å¹¿å·', 'æ·±åœ³', 'ä¸œè', 'ä½›å±±',
    # æ±Ÿè‹çœ
    'æ±Ÿè‹', 'å—äº¬', 'è‹å·', 'æ— é”¡',
    # å±±ä¸œçœ
    'å±±ä¸œ', 'é’å²›', 'æµå—',
    # æµ™æ±Ÿçœ
    'æµ™æ±Ÿ', 'æ­å·', 'å®æ³¢',
    # æ²³å—çœ
    'æ²³å—', 'éƒ‘å·',
    # å››å·çœ
    'å››å·', 'æˆéƒ½',
    # æ¹–åŒ—çœ
    'æ¹–åŒ—', 'æ­¦æ±‰',
    # æ¹–å—çœ
    'æ¹–å—', 'é•¿æ²™',
    # ç¦å»ºçœ
    'ç¦å»º', 'å¦é—¨', 'ç¦å·',
    # å®‰å¾½çœ
    'å®‰å¾½', 'åˆè‚¥',
    # æ²³åŒ—çœ
    'æ²³åŒ—', 'çŸ³å®¶åº„',
    # è¾½å®çœ
    'è¾½å®', 'æ²ˆé˜³', 'å¤§è¿',
    # é™•è¥¿çœ
    'é™•è¥¿', 'è¥¿å®‰',
    # å…¶ä»–çœä»½ (Provinces)
    'æ±Ÿè¥¿', 'é»‘é¾™æ±Ÿ', 'å¹¿è¥¿', 'å±±è¥¿', 'å‰æ—', 'å†…è’™å¤', 
    'è´µå·', 'ç”˜è‚ƒ', 'æ–°ç–†', 'æµ·å—', 'å®å¤', 'é’æµ·', 'è¥¿è—', 
    # æ¸¯æ¾³å° (SARs/Taiwan)
    'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾'
]
COMPREHENSIVE_CITY_LIST = sorted(list(set(COMPREHENSIVE_CITY_LIST))) # æ’åºå¹¶å»é‡

@app.route('/get_city_options', methods=['GET'])
def get_city_options():
    # ğŸ’¡ æ›´æ”¹ï¼šå§‹ç»ˆè¿”å›å®Œæ•´çš„ã€ç¡¬ç¼–ç çš„çœå¸‚åˆ—è¡¨
    return jsonify(COMPREHENSIVE_CITY_LIST)

# --- ç²˜è´´å¼€å§‹ ---
@app.route('/generate_personas', methods=['POST'])
def generate_personas():
    # --- *** NEW: Added top-level error handling *** ---
    try:
        payload = request.get_json(silent=True) or {}

        def to_float_map(obj):
            # ... (internal helper function remains the same)
            result = {}
            if not isinstance(obj, dict): return result
            for key, value in obj.items():
                if key is None: continue
                try: result[str(key)] = float(value)
                except (TypeError, ValueError): result[str(key)] = 0.0
            return result

        try: count = int(payload.get('count', 0))
        except (TypeError, ValueError): count = 0
        if count <= 0: return jsonify({"error": "ç”»åƒæ•°é‡å¿…é¡»å¤§äº 0ã€‚"}), 400

        age_range = payload.get('age_range') or {}
        try:
            age_min = int(age_range.get('min', 0))
            age_max = int(age_range.get('max', 0))
        except (TypeError, ValueError): age_min, age_max = 0, 0
        if age_min < 18 or age_max > 80 or age_min >= age_max:
            return jsonify({"error": "å¹´é¾„èŒƒå›´åº”åœ¨ 18-80 å²ä¹‹é—´ï¼Œä¸”æœ€å°å€¼å°äºæœ€å¤§å€¼ã€‚"}), 400

        gender_ratio = to_float_map(payload.get('gender_ratio') or {})
        drink_ratio = to_float_map(payload.get('drink_frequency_ratio') or {})
        flavor_ratio = to_float_map(payload.get('flavor_ratio') or {})
        mbti_payload = payload.get('mbti_ratio') or {}
        mbti_labels = {'energy': 'èƒ½é‡å€¾å‘', 'info': 'ä¿¡æ¯æ¥æ”¶', 'decision': 'å†³ç­–æ–¹å¼', 'life': 'ç”Ÿæ´»æ€åº¦'}
        mbti_ratio = {
            'energy': to_float_map(mbti_payload.get('energy') or {}),
            'info': to_float_map(mbti_payload.get('info') or {}),
            'decision': to_float_map(mbti_payload.get('decision') or {}),
            'life': to_float_map(mbti_payload.get('life') or {}),
        }
        city_ratio_entries = []
        for entry in payload.get('city_ratio') or []:
            city = entry.get('city')
            if not city: continue
            try: ratio_value = float(entry.get('ratio', 0))
            except (TypeError, ValueError): ratio_value = 0.0
            city_ratio_entries.append({'city': city, 'ratio': ratio_value})

        def ratio_total_close_to_hundred(total): return abs(total - 100) <= 1.5

        # --- Validation Checks (remain the same) ---
        validation_errors = []
        for label, ratio_map in [('æ€§åˆ«', gender_ratio), ('é¥®é…’é¢‘ç‡', drink_ratio), ('åå¥½é¦™å‹', flavor_ratio)]:
            if ratio_map and not ratio_total_close_to_hundred(sum(ratio_map.values())):
                 validation_errors.append(f"{label} æ¯”ä¾‹æ€»å’Œéœ€ä¸º 100ï¼Œå½“å‰ä¸º {sum(ratio_map.values()):.2f}ã€‚")

        for key, ratio_map in mbti_ratio.items():
            if ratio_map and not ratio_total_close_to_hundred(sum(ratio_map.values())):
                label = mbti_labels.get(key, key)
                validation_errors.append(f"MBTI {label} æ¯”ä¾‹æ€»å’Œéœ€ä¸º 100ï¼Œå½“å‰ä¸º {sum(ratio_map.values()):.2f}ã€‚")

        if not city_ratio_entries:
             validation_errors.append("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåŸå¸‚ã€‚")
        elif not ratio_total_close_to_hundred(sum(item['ratio'] for item in city_ratio_entries)):
             validation_errors.append(f"åŸå¸‚æ¯”ä¾‹æ€»å’Œéœ€ä¸º 100ï¼Œå½“å‰ä¸º {sum(item['ratio'] for item in city_ratio_entries):.2f}ã€‚")
        else:
            seen_cities, duplicate_cities = set(), set()
            for item in city_ratio_entries:
                city = item['city']
                if city in seen_cities: duplicate_cities.add(city)
                seen_cities.add(city)
            if duplicate_cities:
                 validation_errors.append(f"åŸå¸‚ {', '.join(sorted(duplicate_cities))} é‡å¤ï¼Œè¯·è°ƒæ•´ã€‚")

        # If there are validation errors, return them now
        if validation_errors:
            return jsonify({"error": "\n".join(validation_errors)}), 400
        # --- End Validation ---

        city_ratio_map = {item['city']: item['ratio'] for item in city_ratio_entries}
        gender_quota = allocate_counts_from_ratio(gender_ratio, count)
        drink_quota = allocate_counts_from_ratio(drink_ratio, count)
        flavor_quota = allocate_counts_from_ratio(flavor_ratio, count)
        city_quota = allocate_counts_from_ratio(city_ratio_map, count)

        def format_counts(label, counts):
            # ... (internal helper function remains the same)
            if not counts: return f"{label}ï¼šæœªæŒ‡å®š"
            return f"{label}ï¼š" + "ã€".join(f"{k} {v}äºº" for k, v in counts.items())

        def format_mbti_ratio():
            # ... (internal helper function remains the same)
            lines = []
            for key, ratio_map in mbti_ratio.items():
                if not ratio_map: continue
                ratio_text = "ã€".join(f"{sub_key} {value:.0f}%" for sub_key, value in ratio_map.items())
                lines.append(f"{mbti_labels.get(key, key)}ï¼š{ratio_text}")
            return "\n".join(lines)

        def build_prompt(target_count):
            # ... (internal helper function remains the same)
            prompt_parts = [
                f"ä½ æ˜¯ä¸€ä½æ¶ˆè´¹è€…æ´å¯Ÿä¸“å®¶ï¼Œè¯·ç”Ÿæˆ {target_count} ä¸ªä¸­å›½ç™½é…’æ¶ˆè´¹è€…ç”»åƒã€‚",
                f"å¹´é¾„éœ€åˆ†å¸ƒåœ¨ {age_min}-{age_max} å²ä¹‹é—´ï¼Œæ¯ä½ç”»åƒçš„å¹´é¾„ä¸ºæ•´æ•°ã€‚",
                format_counts("æ€§åˆ«é…é¢", gender_quota),
                format_counts("åŸå¸‚é…é¢", city_quota),
                format_counts("é¥®é…’é¢‘ç‡é…é¢", drink_quota),
                format_counts("åå¥½é¦™å‹é…é¢", flavor_quota),
                "MBTI å€¾å‘è¯·å°½é‡è´´è¿‘ä»¥ä¸‹æ¯”ä¾‹ï¼š",
                format_mbti_ratio() or "ï¼ˆæœªæä¾›é¢å¤–çº¦æŸï¼Œå¯è‡ªè¡Œåˆç†è®¾å®šï¼‰",
                "æ¯åç”»åƒéœ€åŒ…å«ä»¥ä¸‹å­—æ®µï¼Œå¹¶ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼šgender, age, city, profession, education, income, expected_price, drink_frequency, drinking_history, preferred_aroma, mbtiã€‚",
                "å­—æ®µçº¦æŸï¼š",
                "1. education å–å€¼èŒƒå›´ï¼šé«˜ä¸­åŠä»¥ä¸‹ã€å¤§ä¸“ã€æœ¬ç§‘ã€ç¡•å£«ã€åšå£«ã€‚",
                "2. income å–å€¼èŒƒå›´ï¼š10ä¸‡ä»¥ä¸‹ã€10-20ä¸‡ã€20-50ä¸‡ã€50ä¸‡ä»¥ä¸Šï¼Œå¯é€‚å½“æ‹“å±•ä½†éœ€ç¬¦åˆå¸¸ç†ã€‚",
                "3. expected_price å–å€¼èŒƒå›´ï¼š100å…ƒä»¥ä¸‹ã€100-299å…ƒã€300-999å…ƒã€1000å…ƒä»¥ä¸Šã€‚",
                "4. drink_frequency å¿…é¡»æ¥è‡ªé…é¢ä¸­çš„ç±»åˆ«ã€‚",
                "5. preferred_aroma å¿…é¡»æ¥è‡ªé…é¢ä¸­çš„ç±»åˆ«ã€‚",
                "6. drinking_history ä¸ºæ•´æ•°ï¼ŒèŒƒå›´ 0-30ï¼Œä¸”ä¸å¾—è¶…è¿‡å¹´é¾„ - 18ã€‚",
                "7. MBTI å¿…é¡»æ˜¯å››å­—æ¯ç»„åˆï¼ˆå¦‚ ISTJã€ENFPï¼‰ï¼Œè¯·ç¬¦åˆæ¯”ä¾‹è¦æ±‚ã€‚",
                "8. professionã€educationã€income ç­‰ä¿¡æ¯éœ€ä¿æŒäººç‰©ä¹‹é—´çš„å·®å¼‚ä¸çœŸå®æ€§ã€‚",
                f"è¾“å‡ºæ ¼å¼ï¼šå¿…é¡»è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸”ä»…åŒ…å«ä¸€ä¸ªé”® `personas`ï¼Œå…¶å€¼æ˜¯é•¿åº¦ä¸º {target_count} çš„æ•°ç»„ã€‚ä¸å¾—åŒ…å«é¢å¤–è¯´æ˜ã€ç©ºè¡Œæˆ– Markdown ç¬¦å·ã€‚",
                "ç¡®ä¿ JSON ä¸¥æ ¼ç¬¦åˆ RFC8259ï¼Œæ‰€æœ‰å­—ç¬¦ä¸²ä½¿ç”¨åŒå¼•å·ï¼Œä¸¥ç¦å°¾éšé€—å·ï¼›å¦‚æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¡«å…¥æœ€æ¥è¿‘çš„åˆç†å€¼ã€‚",
                f"åœ¨è¿”å›ç»“æœä¹‹å‰ï¼ŒåŠ¡å¿…æ ¸å¯¹ personas æ•°ç»„é•¿åº¦æ˜¯å¦ç­‰äº {target_count}ï¼›è‹¥ä¸æ»¡è¶³å¿…é¡»é‡å†™å¹¶ä»…åœ¨æ¡ä»¶æ»¡è¶³æ—¶è¿”å›ç»“æœã€‚"
            ]
            return "\n".join(part for part in prompt_parts if part)

        def build_example_payload():
             # Use first city from entries if available, else default
            first_city = city_ratio_entries[0]['city'] if city_ratio_entries else "åŒ—äº¬"
            first_drink_freq = list(drink_quota.keys())[0] if drink_quota else "æ¯æœˆ1-2æ¬¡"
            first_flavor = list(flavor_quota.keys())[0] if flavor_quota else "é…±é¦™å‹"
            first_gender = list(gender_quota.keys())[0] if gender_quota else "ç”·"

            return {"personas": [{"gender": first_gender, "age": age_min, "city": first_city, "profession": "å¸‚åœºæ€»ç›‘", "education": "æœ¬ç§‘", "income": "20-50ä¸‡", "expected_price": "300-999å…ƒ", "drink_frequency": first_drink_freq, "drinking_history": 6, "preferred_aroma": first_flavor, "mbti": "ENTJ"}]}

        def extract_personas(text):
            # ... (internal helper function remains the same)
            errors = []
            def try_load(payload):
                try:
                    data = json.loads(payload)
                    if isinstance(data, dict) and isinstance(data.get("personas"), list): return data["personas"]
                    if isinstance(data, list): return data
                except Exception as exc: errors.append(exc)
                return None
            parsed = try_load(text)
            if parsed is not None: return parsed
            candidate = text.strip()
            if candidate.startswith("```"):
                candidate = candidate.strip('`').split('\n', 1)[-1]
            start, end = candidate.find('['), candidate.rfind(']')
            if start != -1 and end != -1:
                array_slice = candidate[start:end + 1]
                parsed = try_load(f'{{"personas": {array_slice}}}')
                if parsed is not None: return parsed
                parsed = try_load(array_slice)
                if parsed is not None: return parsed
            parsed = try_load(candidate)
            if parsed is not None: return parsed
            sanitized = re.sub(r',\s*([\]}])', r'\1', candidate)
            if sanitized != candidate:
                parsed = try_load(sanitized)
                if parsed is not None: return parsed
                if sanitized.endswith(','):
                    parsed = try_load(sanitized.rstrip(','))
                    if parsed is not None: return parsed
            python_candidate = candidate.replace('null', 'None').replace('true', 'True').replace('false', 'False')
            try:
                data = ast.literal_eval(python_candidate)
                if isinstance(data, dict) and isinstance(data.get("personas"), list): return data["personas"]
                if isinstance(data, list): return data
            except Exception as exc: errors.append(exc)
            raise (errors[0] if errors else ValueError("æ— æ³•è§£æè¿”å›å†…å®¹"))

        def request_persona_batch(target_count):
            # ... (internal helper function remains the same)
            prompt = build_prompt(target_count)
            example_payload = build_example_payload()
            raw_text, last_error = "", None
            for _ in range(3): # Retry loop
                try:
                    response = client.chat.completions.create(model="gpt-4o-mini", temperature=0.2, max_tokens=4000, response_format={"type": "json_object"}, messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æ¶ˆè´¹è€…æ´å¯Ÿä¸“å®¶ï¼Œæ“…é•¿æŒ‰ç…§é…é¢ç”ŸæˆçœŸå®å¯ä¿¡çš„ç”¨æˆ·ç”»åƒã€‚è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ª JSON è§„èŒƒã€‚"}, {"role": "user", "content": f"{prompt}\nè¯·ä¸¥æ ¼å‚ç…§ä»¥ä¸‹ç¤ºä¾‹è¿”å›æ ¼å¼ï¼š{json.dumps(example_payload, ensure_ascii=False)}"}])
                except Exception as exc:
                    last_error = exc
                    print(f"API call failed during persona batch generation: {exc}") 
                    continue

                if not response or not response.choices or not response.choices[0].message or not response.choices[0].message.content:
                    last_error = ValueError("API returned an empty response during persona batch generation.")
                    print(last_error) 
                    continue 

                raw_text = response.choices[0].message.content.strip()
                try: 
                    personas = extract_personas(raw_text)
                except Exception as exc:
                    last_error = exc
                    print(f"Failed to extract personas during batch generation: {exc}") 
                    print(f"Raw text was: {raw_text[:500]}") 
                    continue

                if isinstance(personas, list) and len(personas) == target_count: 
                    return True, personas, raw_text, None

                last_error = ValueError(f"Batch generation expected {target_count}, got {len(personas) if isinstance(personas, list) else 'unknown'}.")
                print(last_error) 

            return False, [], raw_text, last_error


        # --- Persona Generation Loop (remains the same) ---
        all_personas, remaining = [], count
        batch_size = 10 if count > 10 else count
        last_raw_text, last_error = "", None
        while remaining > 0:
            target_count = min(batch_size, remaining)
            success, batch_personas, raw_text, error = request_persona_batch(target_count)
            if not success:
                last_error, last_raw_text = error, raw_text
                break
            all_personas.extend(batch_personas)
            remaining = count - len(all_personas)
        if remaining > 0: return jsonify({"error": f"ç”Ÿæˆç”»åƒå¤±è´¥: {str(last_error)}" if last_error else "ç”Ÿæˆç”»åƒå¤±è´¥", "raw": (last_raw_text or "")[:400]}), 500
        personas = all_personas
        if not isinstance(personas, list) or len(personas) != count:
            return jsonify({"error": f"æœ€ç»ˆæ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {count} ä¸ªï¼Œå®é™…å¾—åˆ° {len(personas) if isinstance(personas, list) else 'æœªçŸ¥'} ä¸ªã€‚", "raw": (last_raw_text or "")[:400]}), 500


        # --- Cleaning and Saving (remains the same) ---
        defaults = {"gender": "æœªæŒ‡å®š", "age": age_min, "city": "æœªæŒ‡å®š", "profession": "æœªæŒ‡å®šèŒä¸š", "education": "æœ¬ç§‘", "income": "10-20ä¸‡", "expected_price": "300-999å…ƒ", "drink_frequency": "æ¯æœˆ1-2æ¬¡", "drinking_history": 0, "preferred_aroma": "é…±é¦™å‹", "mbti": "ISTJ"}
        cleaned_personas = []
        for persona in personas:
            cleaned = {}
            for key, default_value in defaults.items():
                value = persona.get(key, default_value)
                if key in {'gender', 'city', 'profession', 'education', 'income', 'expected_price', 'drink_frequency', 'preferred_aroma'}:
                    cleaned[key] = str(value).strip() if pd.notna(value) else default_value
                elif key == 'age':
                    try: age_val = int(value)
                    except (TypeError, ValueError): age_val = age_min
                    cleaned[key] = max(age_min, min(age_max, age_val))
                elif key == 'drinking_history':
                    try: history = int(value)
                    except (TypeError, ValueError): history = defaults['drinking_history']
                    history = max(0, min(30, history))
                    cleaned[key] = min(history, cleaned.get('age', age_min) - 18 if cleaned.get('age', age_min) > 18 else 0)
                elif key == 'mbti':
                    mbti_val = str(value).strip().upper() if pd.notna(value) else default_value
                    if re.match(r"^[IE][NS][TF][JP]$", mbti_val):
                         cleaned[key] = mbti_val
                    else:
                         cleaned[key] = defaults['mbti'] 

            cleaned_personas.append(cleaned)

        summary = {"gender": Counter(p['gender'] for p in cleaned_personas), "city": Counter(p['city'] for p in cleaned_personas), "drink_frequency": Counter(p['drink_frequency'] for p in cleaned_personas), "preferred_aroma": Counter(p['preferred_aroma'] for p in cleaned_personas)}

        # --- File Saving (Optional - keep as is for now) ---
        # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # output_dir = Path("synthetic_personas") / timestamp
        # output_dir.mkdir(parents=True, exist_ok=True)
        # personas_file = output_dir / "personas.json"
        # config_file = output_dir / "config.json"
        # with personas_file.open('w', encoding='utf-8') as f: json.dump(cleaned_personas, f, ensure_ascii=False, indent=2)
        # with config_file.open('w', encoding='utf-8') as f: json.dump({"count": count, "age_range": {"min": age_min, "max": age_max}, "gender_ratio": gender_ratio, "drink_frequency_ratio": drink_ratio, "flavor_ratio": flavor_ratio, "mbti_ratio": mbti_ratio, "city_ratio": city_ratio_entries}, f, ensure_ascii=False, indent=2)

        return jsonify({"personas": cleaned_personas, "file": None, "summary": {key: dict(value) for key, value in summary.items()}})
    # --- *** Outer Error Catching Block *** ---
    except Exception as e:
        # Log the detailed error to the server console
        print(f"!!! Error in /generate_personas endpoint !!!")
        import traceback
        traceback.print_exc() 
        # Return a generic error message to the user
        return jsonify({"error": f"ç”Ÿæˆç”»åƒæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500
# --- ç²˜è´´ç»“æŸ ---


# --- Flask Routes ---
@app.route('/generate_report', methods=['POST'])
def generate_report_start():
    job_id = hashlib.md5(str(time.time()).encode()).hexdigest()
    analysis_queues[job_id] = Queue()
    data = request.json
    thread = threading.Thread(
        target=long_running_analysis,
        args=(job_id, data.get('personas', []), data.get('productData', {}), data.get('persona_file'))
    )
    thread.start()
    return jsonify({"job_id": job_id})

@app.route('/stream/<job_id>')
def stream(job_id):
    def event_stream():
        q = analysis_queues.get(job_id)
        if not q: yield f"data: {json.dumps({'type': 'error', 'data': 'æ— æ•ˆçš„ä»»åŠ¡ID'})}\n\n"; return
        while True:
            try:
                message = q.get(timeout=240)
                yield f"data: {json.dumps(message)}\n\n"
                if message.get('type') == 'done': break
            except Exception:
                yield f"data: {json.dumps({'type': 'error', 'data': 'æ•°æ®æµè¶…æ—¶æˆ–ä»»åŠ¡å·²ç»“æŸ'})}\n\n"; break
    return Response(event_stream(), mimetype='text/event-stream')

@app.route('/')
def index():
    return render_template('index.html')

# --- ### MODIFIED: Final startup logic for Waitress ### ---

# 1. Load and process the real survey data as soon as the app starts
load_and_preprocess_data()

# 2. Build the vector database from the real data
vectorize_database()

# 3. This block is now just for informational purposes when running the script directly
if __name__ == '__main__':
    print("\n--- MingYin Insight Engine ---")
    print("çœŸå®æ•°æ®å·²åŠ è½½å¹¶å‘é‡åŒ–ã€‚åº”ç”¨å·²å‡†å¤‡å°±ç»ªã€‚")
    print("è¯·ä½¿ç”¨ Waitress å¯åŠ¨ç”Ÿäº§æœåŠ¡å™¨ã€‚")
    print("æ¨èå‘½ä»¤: waitress-serve --host=0.0.0.0 --port=5001 --threads=10 app:app")
    pass